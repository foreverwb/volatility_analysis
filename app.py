"""

Flask ä¸»åº”ç”¨å…¥å£

"""
from flask import Flask, render_template, request, jsonify, send_from_directory, Response, stream_with_context
import json
import os
from typing import List, Dict, Any
from datetime import datetime, timedelta, time
from collections import defaultdict
from queue import Queue, Empty
import threading
from core.market_data import get_vix_info, clear_vix_cache

from core import (
    DEFAULT_CFG,
    calculate_analysis
)
from core.oi_fetcher import batch_fetch_oi, auto_tune_workers, estimate_fetch_time
from core.futu_option_iv import fetch_iv_term_structure
from core.background_tasks import (
    get_task_manager, 
    create_iv_fetch_task,
    execute_iv_fetch_task,
    TaskStatus
)
app = Flask(__name__)

DATA_FILE = 'analysis_records.json'

# =========================
# æ—¶é—´åˆ¤æ–­å·¥å…·å‡½æ•°
# =========================
def should_skip_oi_fetch() -> bool:
    """
    åˆ¤æ–­å½“å‰æ—¶é—´æ˜¯å¦åº”è·³è¿‡ OI æ•°æ®è·å–
    
    è§„åˆ™: åŒ—äº¬æ—¶é—´ 18:00 ä¹‹å‰è·³è¿‡
    
    Returns:
        True if å½“å‰æ—¶é—´ < 18:00 CST
    """
    import pytz
    
    beijing_tz = pytz.timezone('Asia/Shanghai')
    now_beijing = datetime.now(beijing_tz)
    
    cutoff_time = time(18, 0, 0)
    
    return now_beijing.time() < cutoff_time


# =========================
# æ•°æ®æŒä¹…åŒ–
# =========================
def load_data() -> List[Dict[str, Any]]:
    """åŠ è½½åˆ†æè®°å½•"""
    if not os.path.exists(DATA_FILE):
        return []
    try:
        with open(DATA_FILE, 'r', encoding='utf-8') as f:
            content = f.read().strip()
            if not content:
                return []
            data = json.loads(content)
            return data if isinstance(data, list) else []
    except (json.JSONDecodeError, Exception) as e:
        print(f"è­¦å‘Š: è¯»å– {DATA_FILE} å¤±è´¥: {e}")
        return []


def save_data(data: List[Dict[str, Any]]):
    """ä¿å­˜åˆ†æè®°å½•"""
    with open(DATA_FILE, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


def get_history_scores(symbol: str, n_days: int = 5, as_of_date: str = None) -> List[float]:
    """
    è·å–æŒ‡å®šæ ‡çš„çš„å†å²æ–¹å‘è¯„åˆ†ï¼ˆç”¨äºè·¨æœŸä¸€è‡´æ€§è®¡ç®—ï¼‰
    
    Args:
        symbol: æ ‡çš„ä»£ç ï¼ˆå¤§å°å†™ä¸æ•æ„Ÿï¼‰
        n_days: éœ€è¦çš„å†å²å¤©æ•°ï¼ˆé»˜è®¤ 5 å¤©ï¼‰
        as_of_date: æˆªæ­¢æ—¥æœŸï¼ˆæ ¼å¼: YYYY-MM-DDï¼‰ï¼Œé»˜è®¤ä¸ºä»Šå¤©
        
    Returns:
        å†å²è¯„åˆ†åˆ—è¡¨ï¼ˆæŒ‰æ—¶é—´å€’åºï¼Œæœ€æ–°åœ¨å‰ï¼‰
    """
    records = load_data()
    symbol_upper = symbol.upper()
    
    symbol_records = [
        r for r in records 
        if r.get('symbol', '').upper() == symbol_upper
    ]
    
    if not symbol_records:
        return []
    
    if as_of_date is None:
        as_of = datetime.now()
    else:
        try:
            as_of = datetime.strptime(as_of_date, '%Y-%m-%d')
        except ValueError:
            as_of = datetime.now()
    
    records_by_date = defaultdict(list)
    
    for r in symbol_records:
        timestamp = r.get('timestamp', '')
        if not timestamp:
            continue
        
        try:
            date_str = timestamp.split(' ')[0]
            dt = datetime.strptime(date_str, '%Y-%m-%d')
            
            if dt <= as_of:
                records_by_date[date_str].append(r)
        except (ValueError, IndexError):
            continue
    
    daily_latest = {}
    for date_str, day_records in records_by_date.items():
        day_records.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
        daily_latest[date_str] = day_records[0]
    
    sorted_dates = sorted(daily_latest.keys(), reverse=True)
    
    history_scores = []
    for date_str in sorted_dates[:n_days]:
        record = daily_latest[date_str]
        score = record.get('direction_score', 0)
        history_scores.append(float(score))
    
    return history_scores


# =========================
# Flask è·¯ç”±
# =========================
@app.route('/static/<path:filename>')
def serve_static(filename):
    return send_from_directory('static', filename)


@app.route('/')
def index():
    return render_template('index.html')


@app.route('/api/analyze', methods=['POST'])
def analyze():
    """    
    POST /api/analyze?ignore_earnings=false
    Body: { "records": [...] }
    
    âœ¨ ä¼˜åŒ–: å…ˆè·å– OIï¼ˆå¿«ï¼‰â†’ å†è·å– IVï¼ˆæ…¢ï¼‰
    """
    try:
        ignore_earnings = request.args.get('ignore_earnings', 'false').lower() == 'true'
        records = request.json.get('records', [])
        
        if not isinstance(records, list):
            return jsonify({'error': 'æ•°æ®æ ¼å¼é”™è¯¯,éœ€è¦æ˜¯åˆ—è¡¨'}), 400
        
        if len(records) == 0:
            return jsonify({'error': 'æ•°æ®åˆ—è¡¨ä¸èƒ½ä¸ºç©º'}), 400
        
        skip_oi = should_skip_oi_fetch()
        
        symbols = list(set(r.get('symbol', '') for r in records if r.get('symbol')))
        num_symbols = len(symbols)
        
        # ========== âœ¨ ä¼˜åŒ–ï¼šå…ˆå¿«åæ…¢ ==========
        
        # 1ï¸âƒ£ å…ˆè·å– OI æ•°æ®ï¼ˆå¿«ï¼Œä¸”ç”¨æˆ·æœ€å…³å¿ƒï¼‰
        oi_data = {}
        
        if skip_oi:
            print(f"\nâ° å½“å‰æ—¶é—´æ—©äº 18:00 CSTï¼Œè·³è¿‡ OI æ•°æ®è·å–")
        else:
            auto_tuned_workers = auto_tune_workers(num_symbols)
            estimated_time = estimate_fetch_time(num_symbols, auto_tuned_workers)
            
            print(f"\n{'='*60}")
            print(f"ğŸ“Š OI æ•°æ®è·å–é…ç½®:")
            print(f"   - æ ‡çš„æ•°é‡: {num_symbols}")
            print(f"   - å¹¶å‘çº¿ç¨‹: {auto_tuned_workers}")
            print(f"   - é¢„è®¡è€—æ—¶: {estimated_time:.1f}s")
            print(f"{'='*60}\n")
            
            oi_data = batch_fetch_oi(symbols, max_workers=auto_tuned_workers)
        
        # 2ï¸âƒ£ å†è·å– IV æ•°æ®ï¼ˆæ…¢ï¼Œä½†å¯ä»¥å¹¶å‘ï¼‰
        print(f"\n{'='*60}")
        print(f"ğŸ“ˆ IV æ•°æ®è·å–é…ç½®:")
        print(f"   - æ ‡çš„æ•°é‡: {num_symbols}")
        print(f"   - å¹¶å‘çº¿ç¨‹: 5 (Futu é™æµä¿æŠ¤)")
        print(f"{'='*60}\n")
        
        iv_term_data = fetch_iv_term_structure(
            symbols,
            max_workers=5  # Futu API é™æµï¼Œå»ºè®®ä¸è¶…è¿‡5
        )
        
        # 3ï¸âƒ£ åˆ†ææ•°æ®
        results = []
        errors = []
        
        for i, record in enumerate(records):
            try:
                symbol = record.get('symbol', '')
                symbol_upper = symbol.upper()

                # æ³¨å…¥ IV æ•°æ®
                if symbol_upper in iv_term_data:
                    iv_values = iv_term_data[symbol_upper]
                    for key, value in iv_values.items():
                        if value is not None:
                            record[key] = value
                    if iv_values.get("IV_90D") is not None:
                        record["IV90"] = iv_values["IV_90D"]
                
                # æ³¨å…¥ OI æ•°æ®
                if not skip_oi and symbol in oi_data:
                    current_oi, delta_oi = oi_data[symbol]
                    if delta_oi is not None:
                        record['Î”OI_1D'] = delta_oi
                        
                history_scores = get_history_scores(symbol)
                
                analysis = calculate_analysis(
                    record,
                    ignore_earnings=ignore_earnings,
                    history_scores=history_scores,
                    skip_oi=skip_oi
                )
                results.append(analysis)
            except Exception as e:
                error_msg = f"æ ‡çš„ {record.get('symbol', f'#{i+1}')} åˆ†æå¤±è´¥: {str(e)}"
                errors.append(error_msg)
                print(f"é”™è¯¯: {error_msg}")
        
        # 4ï¸âƒ£ ä¿å­˜æ•°æ®
        if results:
            all_data = load_data()
            new_records_map = {}
            for r in results:
                date = r['timestamp'].split(' ')[0]
                symbol = r['symbol']
                key = (date, symbol)
                new_records_map[key] = r
            
            filtered_old_data = []
            for old_record in all_data:
                date = old_record.get('timestamp', '').split(' ')[0]
                symbol = old_record.get('symbol', '')
                key = (date, symbol)
                if key not in new_records_map:
                    filtered_old_data.append(old_record)
            
            all_data = filtered_old_data + results
            save_data(all_data)
        
        message = f'æˆåŠŸåˆ†æ {len(results)} ä¸ªæ ‡çš„'
        if errors:
            message += f',{len(errors)} ä¸ªå¤±è´¥'
        
        if skip_oi:
            message += ' (å·²è·³è¿‡ OI æ•°æ®è·å–)'
        
        return jsonify({
            'message': message,
            'results': results,
            'errors': errors if errors else None,
            'oi_stats': {
                'total': num_symbols,
                'success': sum(1 for s in symbols if oi_data.get(s, (None, None))[0] is not None),
                'with_delta': sum(1 for s in symbols if oi_data.get(s, (None, None))[1] is not None),
                'skipped': skip_oi
            }
        }), 201
    
    except Exception as e:
        print(f"åˆ†æå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return jsonify({'error': str(e)}), 500


@app.route('/api/records', methods=['GET'])
def get_records():
    """è·å–åˆ†æè®°å½•"""
    try:
        data = load_data()
        if not isinstance(data, list):
            return jsonify([])
        
        date_filter = request.args.get('date')
        quadrant_filter = request.args.get('quadrant')
        confidence_filter = request.args.get('confidence')
        
        filtered_data = data
        
        if date_filter:
            filtered_data = [d for d in filtered_data if d.get('timestamp', '').startswith(date_filter)]
        
        if quadrant_filter and quadrant_filter != 'all':
            filtered_data = [d for d in filtered_data if d.get('quadrant') == quadrant_filter]
        
        if confidence_filter and confidence_filter != 'all':
            filtered_data = [d for d in filtered_data if d.get('confidence') == confidence_filter]
        
        return jsonify(filtered_data)
    
    except Exception as e:
        return jsonify([])


@app.route('/api/records/<timestamp>/<symbol>', methods=['DELETE'])
def delete_record(timestamp, symbol):
    """åˆ é™¤å•æ¡è®°å½•"""
    try:
        data = load_data()
        original_length = len(data)
        data = [d for d in data if not (d['timestamp'] == timestamp and d['symbol'] == symbol)]
        if len(data) == original_length:
            return jsonify({'error': 'æœªæ‰¾åˆ°è¯¥è®°å½•'}), 404
        save_data(data)
        return jsonify({'message': 'åˆ é™¤æˆåŠŸ'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/records/date/<date>', methods=['DELETE'])
def delete_records_by_date(date):
    """æŒ‰æ—¥æœŸåˆ é™¤è®°å½•"""
    try:
        data = load_data()
        original_length = len(data)
        data = [d for d in data if not d.get('timestamp', '').startswith(date)]
        deleted_count = original_length - len(data)
        if deleted_count == 0:
            return jsonify({'error': 'æœªæ‰¾åˆ°è¯¥æ—¥æœŸçš„è®°å½•'}), 404
        save_data(data)
        return jsonify({'message': f'æˆåŠŸåˆ é™¤ {deleted_count} æ¡è®°å½•'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/records/all', methods=['DELETE'])
def delete_all_records():
    """åˆ é™¤æ‰€æœ‰è®°å½•"""
    try:
        save_data([])
        return jsonify({'message': 'æ‰€æœ‰æ•°æ®å·²åˆ é™¤'}), 200
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/dates', methods=['GET'])
def get_dates():
    """è·å–æ‰€æœ‰æ—¥æœŸ"""
    try:
        data = load_data()
        dates = sorted(set(d.get('timestamp', '')[:10] for d in data if d.get('timestamp')), reverse=True)
        return jsonify(dates)
    except Exception as e:
        return jsonify([]), 200


@app.route('/api/config', methods=['GET'])
def get_config():
    """è·å–å½“å‰é…ç½®"""
    return jsonify(DEFAULT_CFG)


@app.route('/api/config', methods=['POST'])
def update_config():
    """æ›´æ–°é…ç½® (è¿è¡Œæ—¶)"""
    try:
        new_cfg = request.json
        DEFAULT_CFG.update(new_cfg)
        return jsonify({'message': 'é…ç½®æ›´æ–°æˆåŠŸ', 'config': DEFAULT_CFG})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/analyze/stream', methods=['POST'])
def analyze_stream():
    """
    æµå¼åˆ†ææ¥å£ - v2.6.0 å¼‚æ­¥ä¼˜åŒ–ç‰ˆ
    
    âœ¨ ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä¼˜å…ˆè·å– Î”OIï¼ˆå¿«ï¼Œ~30ç§’ï¼‰
    2. ä½¿ç”¨ç°æœ‰ IV æ•°æ®è¿›è¡Œåˆæ­¥åˆ†æ
    3. ç«‹å³è¿”å›ç»“æœç»™ç”¨æˆ·
    4. åå°å¯åŠ¨ IV è·å–ä»»åŠ¡
    5. IV å®Œæˆåæ¨é€æ›´æ–°é€šçŸ¥
    
    POST /api/analyze/stream?ignore_earnings=false&async_iv=true
    Body: { "records": [...] }
    """
    def generate():
        try:
            ignore_earnings = request.args.get('ignore_earnings', 'false').lower() == 'true'
            async_iv = request.args.get('async_iv', 'true').lower() == 'true'  # âœ¨ æ–°å¢å‚æ•°
            records = request.json.get('records', [])
            
            if not isinstance(records, list) or len(records) == 0:
                yield f"data: {json.dumps({'type': 'error', 'error': 'æ•°æ®æ ¼å¼é”™è¯¯'})}\n\n"
                return
            
            symbols = list(set(r.get('symbol', '') for r in records if r.get('symbol')))
            num_symbols = len(symbols)
            
            yield f"data: {json.dumps({'type': 'init', 'total': num_symbols, 'async_iv': async_iv})}\n\n"
            
            skip_oi = should_skip_oi_fetch()
            
            # ========== 1ï¸âƒ£ ä¼˜å…ˆè·å– OI æ•°æ®ï¼ˆå¿«ï¼‰ ==========
            oi_data = {}
            
            if skip_oi:
                info_msg = {'type': 'info', 'message': 'å½“å‰æ—¶é—´æ—©äº 18:00 CSTï¼Œè·³è¿‡ OI æ•°æ®è·å–'}
                yield f"data: {json.dumps(info_msg)}\n\n"
                yield f"data: {json.dumps({'type': 'oi_complete', 'success': 0, 'skipped': True})}\n\n"
            else:
                auto_tuned_workers = auto_tune_workers(num_symbols)
                estimated_time = estimate_fetch_time(num_symbols, auto_tuned_workers)
                
                yield f"data: {json.dumps({'type': 'oi_start', 'estimated_time': estimated_time})}\n\n"
                
                progress_queue = Queue()
                fetch_error = None
                
                def fetch_oi_task():
                    nonlocal oi_data, fetch_error
                    try:
                        oi_data = batch_fetch_oi(
                            symbols, 
                            max_workers=auto_tuned_workers,
                            progress_queue=progress_queue
                        )
                    except Exception as e:
                        fetch_error = str(e)
                        progress_queue.put({'type': 'error', 'error': str(e)})
                
                fetch_thread = threading.Thread(target=fetch_oi_task)
                fetch_thread.start()
                
                oi_fetch_complete = False
                
                while not oi_fetch_complete or not progress_queue.empty():
                    try:
                        progress_data = progress_queue.get(timeout=0.5)
                        
                        if progress_data.get('type') == 'complete':
                            oi_fetch_complete = True
                            complete_data = {
                                'type': 'oi_complete', 
                                'success': sum(1 for s in symbols if oi_data.get(s, (None, None))[0] is not None),
                                'skipped': False
                            }
                            yield f"data: {json.dumps(complete_data)}\n\n"
                            break
                        
                        elif progress_data.get('type') == 'error':
                            yield f"data: {json.dumps(progress_data)}\n\n"
                            return
                        
                        else:
                            progress_msg = {
                                'type': 'oi_progress',
                                'completed': progress_data['completed'],
                                'total': progress_data['total'],
                                'symbol': progress_data['symbol']
                            }
                            yield f"data: {json.dumps(progress_msg)}\n\n"
                    
                    except Empty:
                        if not fetch_thread.is_alive():
                            oi_fetch_complete = True
                            break
                        continue
                
                fetch_thread.join(timeout=5)
                
                if fetch_error:
                    yield f"data: {json.dumps({'type': 'error', 'error': fetch_error})}\n\n"
                    return
            
            # ========== 2ï¸âƒ£ ä½¿ç”¨ç°æœ‰ IV æ•°æ®è¿›è¡Œåˆæ­¥åˆ†æ ==========
            yield f"data: {json.dumps({'type': 'info', 'message': 'ä½¿ç”¨ç°æœ‰ IV æ•°æ®è¿›è¡Œåˆ†æ...'})}\n\n"
            
            # æ³¨æ„ï¼šä¸ä¸»åŠ¨è·å–æ–° IVï¼Œåªç”¨ records ä¸­å·²æœ‰çš„
            results = []
            errors = []
            
            for i, record in enumerate(records):
                try:
                    symbol = record.get('symbol', '')
                    
                    # æ³¨å…¥ OI æ•°æ®
                    if not skip_oi and symbol in oi_data:
                        current_oi, delta_oi = oi_data[symbol]
                        if delta_oi is not None:
                            record['Î”OI_1D'] = delta_oi
                    
                    history_scores = get_history_scores(symbol)
                    
                    analysis = calculate_analysis(
                        record,
                        ignore_earnings=ignore_earnings,
                        history_scores=history_scores,
                        skip_oi=skip_oi
                    )
                    results.append(analysis)
                    
                    if i % 5 == 0 or i == len(records) - 1:
                        yield f"data: {json.dumps({
                            'type': 'analyze_progress', 
                            'completed': i + 1, 
                            'total': len(records)
                        })}\n\n"
                    
                except Exception as e:
                    error_msg = f"æ ‡çš„ {record.get('symbol', f'#{i+1}')} åˆ†æå¤±è´¥: {str(e)}"
                    errors.append(error_msg)
            
            # ========== 3ï¸âƒ£ ä¿å­˜åˆæ­¥ç»“æœ ==========
            if results:
                all_data = load_data()
                new_records_map = {}
                for r in results:
                    date = r['timestamp'].split(' ')[0]
                    symbol = r['symbol']
                    key = (date, symbol)
                    new_records_map[key] = r
                
                filtered_old_data = []
                for old_record in all_data:
                    date = old_record.get('timestamp', '').split(' ')[0]
                    symbol = old_record.get('symbol', '')
                    key = (date, symbol)
                    if key not in new_records_map:
                        filtered_old_data.append(old_record)
                
                all_data = filtered_old_data + results
                save_data(all_data)
            
            message = f'âœ“ åˆæ­¥åˆ†æå®Œæˆ {len(results)} ä¸ªæ ‡çš„'
            if errors:
                message += f', {len(errors)} ä¸ªå¤±è´¥'
            if skip_oi:
                message += ' (å·²è·³è¿‡ OI æ•°æ®è·å–)'
            
            # ========== 4ï¸âƒ£ è¿”å›åˆæ­¥ç»“æœ ==========
            initial_result = {
                'type': 'analysis_complete',
                'message': message,
                'results': results,
                'errors': errors if errors else None,
                'oi_stats': {
                    'total': num_symbols,
                    'success': sum(1 for s in symbols if oi_data.get(s, (None, None))[0] is not None),
                    'with_delta': sum(1 for s in symbols if oi_data.get(s, (None, None))[1] is not None),
                    'skipped': skip_oi
                }
            }
            
            yield f"data: {json.dumps(initial_result)}\n\n"
            
            # ========== 5ï¸âƒ£ å¯åŠ¨åå° IV è·å–ä»»åŠ¡ ==========
            if async_iv:
                # è·å–éœ€è¦æ›´æ–° IV çš„ symbolsï¼ˆIV æ•°æ®ç¼ºå¤±æˆ–è¿‡æœŸï¼‰
                symbols_need_iv = []
                for record in records:
                    symbol = record.get('symbol', '')
                    iv30 = record.get('IV30') or record.get('IV_30D')
                    if iv30 is None or iv30 == 0:
                        symbols_need_iv.append(symbol)
                
                if symbols_need_iv:
                    # åˆ›å»ºåå°ä»»åŠ¡
                    task_manager = get_task_manager()
                    
                    def on_iv_complete(task_id, iv_results):
                        """IV è·å–å®Œæˆåçš„å›è°ƒ"""
                        print(f"\nğŸ‰ IV ä»»åŠ¡å®Œæˆ: {task_id}")
                        print(f"   æˆåŠŸè·å–: {sum(1 for data in iv_results.values() if data.get('IV_30D'))} symbols")
                        
                        # é‡æ–°åˆ†æå¹¶æ›´æ–°æ•°æ®
                        updated_records = []
                        for record in records:
                            symbol = record.get('symbol', '').upper()
                            if symbol in iv_results:
                                iv_data = iv_results[symbol]
                                for key, value in iv_data.items():
                                    if value is not None:
                                        record[key] = value
                            
                            try:
                                analysis = calculate_analysis(
                                    record,
                                    ignore_earnings=ignore_earnings,
                                    history_scores=get_history_scores(symbol),
                                    skip_oi=skip_oi
                                )
                                updated_records.append(analysis)
                            except Exception as e:
                                print(f"âš  é‡æ–°åˆ†æå¤±è´¥ {symbol}: {e}")
                        
                        # ä¿å­˜æ›´æ–°åçš„æ•°æ®
                        if updated_records:
                            all_data = load_data()
                            new_records_map = {}
                            for r in updated_records:
                                date = r['timestamp'].split(' ')[0]
                                symbol = r['symbol']
                                key = (date, symbol)
                                new_records_map[key] = r
                            
                            filtered_old_data = []
                            for old_record in all_data:
                                date = old_record.get('timestamp', '').split(' ')[0]
                                symbol = old_record.get('symbol', '')
                                key = (date, symbol)
                                if key not in new_records_map:
                                    filtered_old_data.append(old_record)
                            
                            all_data = filtered_old_data + updated_records
                            save_data(all_data)
                    
                    task_id = create_iv_fetch_task(symbols_need_iv, on_complete=on_iv_complete)
                    
                    # å¯åŠ¨åå°æ‰§è¡Œ
                    execute_iv_fetch_task(task_id, symbols_need_iv)
                    
                    # é€šçŸ¥å‰ç«¯ä»»åŠ¡å·²åˆ›å»º
                    from core.futu_option_iv import FutuBatchController
                    controller = FutuBatchController()
                    batch_config = controller.calculate_batch_config(len(symbols_need_iv))
                    
                    yield f"data: {json.dumps({
                        'type': 'iv_task_created',
                        'task_id': task_id,
                        'symbols_count': len(symbols_need_iv),
                        'estimated_time': batch_config.estimated_time,
                        'message': f'åå°è·å– IV æ•°æ®ä¸­... (é¢„è®¡ {batch_config.estimated_time/60:.1f} åˆ†é’Ÿ)'
                    })}\n\n"
                else:
                    yield f"data: {json.dumps({
                        'type': 'info',
                        'message': 'æ‰€æœ‰æ ‡çš„å·²æœ‰ IV æ•°æ®ï¼Œæ— éœ€åå°æ›´æ–°'
                    })}\n\n"
            
        except Exception as e:
            import traceback
            traceback.print_exc()
            yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive'
        }
    )


# ========== 3. æ–°å¢ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æ¥å£ ==========

@app.route('/api/tasks/<task_id>', methods=['GET'])
def get_task_status(task_id):
    """
    æŸ¥è¯¢åå°ä»»åŠ¡çŠ¶æ€
    
    GET /api/tasks/{task_id}
    
    Returns:
        {
            "task_id": "...",
            "status": "running" | "completed" | "failed",
            "progress": 45,
            "completed_symbols": 15,
            "total_symbols": 32,
            "created_at": "...",
            "completed_at": "..."
        }
    """
    try:
        task_manager = get_task_manager()
        task = task_manager.get_task(task_id)
        
        if not task:
            return jsonify({'error': 'Task not found'}), 404
        
        return jsonify(task.to_dict())
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/tasks', methods=['GET'])
def list_tasks():
    """
    åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡
    
    GET /api/tasks?status=running
    """
    try:
        task_manager = get_task_manager()
        tasks = task_manager.get_all_tasks()
        
        # è¿‡æ»¤ï¼ˆå¯é€‰ï¼‰
        status_filter = request.args.get('status')
        if status_filter:
            tasks = [t for t in tasks if t.status.value == status_filter]
        
        return jsonify([t.to_dict() for t in tasks])
    
    except Exception as e:
        return jsonify({'error': str(e)}), 500


# ========== 4. æ–°å¢ä»»åŠ¡å®Œæˆé€šçŸ¥ï¼ˆSSE æ¨é€ï¼‰ ==========

@app.route('/api/tasks/<task_id>/stream', methods=['GET'])
def stream_task_status(task_id):
    """
    å®æ—¶æ¨é€ä»»åŠ¡çŠ¶æ€ï¼ˆSSEï¼‰
    
    GET /api/tasks/{task_id}/stream
    
    å‰ç«¯å¯ä»¥è®¢é˜…æ­¤æ¥å£ï¼Œå®æ—¶æ¥æ”¶ä»»åŠ¡æ›´æ–°
    """
    def generate():
        task_manager = get_task_manager()
        task = task_manager.get_task(task_id)
        
        if not task:
            yield f"data: {json.dumps({'type': 'error', 'error': 'Task not found'})}\n\n"
            return
        
        # å‘é€åˆå§‹çŠ¶æ€
        yield f"data: {json.dumps({'type': 'status', 'data': task.to_dict()})}\n\n"
        
        # è½®è¯¢ä»»åŠ¡çŠ¶æ€ï¼ˆæ¯2ç§’æ£€æŸ¥ä¸€æ¬¡ï¼‰
        while task.status in (TaskStatus.PENDING, TaskStatus.RUNNING):
            time.sleep(2)
            task = task_manager.get_task(task_id)
            
            if task:
                yield f"data: {json.dumps({'type': 'status', 'data': task.to_dict()})}\n\n"
        
        # ä»»åŠ¡å®Œæˆ
        if task:
            yield f"data: {json.dumps({'type': 'complete', 'data': task.to_dict()})}\n\n"
    
    return Response(
        stream_with_context(generate()),
        mimetype='text/event-stream',
        headers={
            'Cache-Control': 'no-cache',
            'X-Accel-Buffering': 'no',
            'Connection': 'keep-alive'
        }
    )


@app.route('/api/vix/info', methods=['GET'])
def get_vix_cache_info():
    """è·å– VIX ç¼“å­˜çŠ¶æ€"""
    try:
        info = get_vix_info()
        return jsonify(info)
    except Exception as e:
        return jsonify({'error': str(e)}), 500


@app.route('/api/vix/clear', methods=['POST'])
def clear_vix_cache_endpoint():
    """æ¸…é™¤ VIX ç¼“å­˜"""
    try:
        clear_vix_cache()
        return jsonify({'message': 'VIX cache cleared successfully'})
    except Exception as e:
        return jsonify({'error': str(e)}), 500

# æ³¨å†Œ swing é¡¹ç›®çš„ API æ‰©å±•
from api_extension import register_swing_api
register_swing_api(app)


if __name__ == '__main__':
    print("\n" + "="*80)
    print("æœŸæƒç­–ç•¥é‡åŒ–åˆ†æç³»ç»Ÿ v2.6.0 - å¼‚æ­¥ä¼˜åŒ–ç‰ˆ")
    print("="*80)
    print("\nğŸ“¡ API ç«¯ç‚¹:")
    print("   POST /api/analyze/stream      - æµå¼åˆ†ææ¥å£ï¼ˆå¼‚æ­¥IVï¼‰")
    print("   GET  /api/tasks/<task_id>     - æŸ¥è¯¢ä»»åŠ¡çŠ¶æ€")
    print("   GET  /api/tasks               - åˆ—å‡ºæ‰€æœ‰ä»»åŠ¡")
    print("   GET  /api/tasks/<task_id>/stream - å®æ—¶æ¨é€ä»»åŠ¡çŠ¶æ€")
    print("   â€¢ Î”OI ä¼˜å…ˆè·å–ï¼ˆ~30ç§’ï¼‰")
    print("   â€¢ ç«‹å³è¿”å›åˆæ­¥åˆ†æç»“æœ")
    print("   â€¢ IV æ•°æ®åå°å¼‚æ­¥æ›´æ–°ï¼ˆ~2åˆ†é’Ÿï¼‰")
    print("   â€¢ æ”¯æŒä»»åŠ¡çŠ¶æ€å®æ—¶æŸ¥è¯¢")
    print("\nâ° æ—¶é—´é™åˆ¶:")
    print("   â€¢ 18:00 CST ä¹‹å‰è·³è¿‡ OI æ•°æ®è·å–")
    print("="*80 + "\n")
    
    # æ¸…ç†æ—§ä»»åŠ¡
    task_manager = get_task_manager()
    task_manager.cleanup_old_tasks(max_age_hours=24)
    
    try:
        app.run(debug=True, host='0.0.0.0', port=8668)
    except Exception as e:
        print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()